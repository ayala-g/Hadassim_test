{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***חלק א***"
      ],
      "metadata": {
        "id": "fiPHhrqtymg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**סעיף א**"
      ],
      "metadata": {
        "id": "yd-RPlLCyury"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OEyRzDlOFLLl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#התחברות לדרייב לשימוש בקבצים\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at_mhel9F6XT",
        "outputId": "addb7956-6fc9-4f6c-c4d0-59de2ac8af2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/drive/MyDrive/hadasim task/logs.txt.xlsx'"
      ],
      "metadata": {
        "id": "UlhQOqoqJzQ1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#מספר השורות בחלקים שאליהם נחלק את הקובץ\n",
        "slice_size = 1000\n",
        "start_row = 0\n",
        "\n",
        "# יצירת סדרת נתונים ריקה לאיחוד השכיחויות של החלקים השונים\n",
        "merged_frequencies = pd.Series(dtype=int)\n",
        "\n",
        "# לולאה שתרוץ עד שנעבור על כל הקובץ\n",
        "while True:\n",
        "    try:\n",
        "        #קריאה של חלק מתוך הקובץ\n",
        "        slice = pd.read_excel(file_path, sheet_name=\"גיליון1\", header=None,\n",
        "                              skiprows=start_row, nrows=slice_size)\n",
        "\n",
        "        # עצירה כשאין יותר שורות לקרוא\n",
        "        if slice.empty:\n",
        "            break\n",
        "\n",
        "        # חילוץ קוד השגיאה מכל שורה\n",
        "        errors_codes = slice.iloc[:, 0].str.split(\"Error:\").str[1].str.strip()\n",
        "\n",
        "        # ספירת שכיחויות של קודי שגיאה\n",
        "        frequencies = errors_codes.value_counts()\n",
        "\n",
        "        # איחוד שכיחויות מהחלק הנוכחי עם שאר השכיחויות\n",
        "        merged_frequencies = merged_frequencies.add(frequencies, fill_value=0)\n",
        "\n",
        "        # עדכון מיקום התחלה לקריאה הבאה\n",
        "        start_row += slice_size\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"שגיאה בקריאה או עיבוד:\", e)\n",
        "        break\n",
        "\n",
        "# הצגת התוצאה\n",
        "N = 15\n",
        "top_errors = merged_frequencies.sort_values(ascending=False).head(N)\n",
        "\n",
        "# המרה ל-DataFrame והוספת כותרות בעברית\n",
        "top_errors_df = pd.DataFrame({\n",
        "    'קוד שגיאה': top_errors.index,\n",
        "    'שכיחות': top_errors.values\n",
        "})\n",
        "# הדפסה\n",
        "print(top_errors_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Z8Fei0FC43",
        "outputId": "cb5d6ee2-c3f4-4ae0-8f06-0b143f0d76db"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "קוד שגיאה   שכיחות\n",
            " WARN_101 200098.0\n",
            "  ERR_404 200094.0\n",
            "  ERR_400 200069.0\n",
            " INFO_200 199931.0\n",
            "  ERR_500 199808.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **סיבוכיות:**\n",
        "**זמן:** O(n) עוברים על השורות אחת אחת בלי ביקה כפולה\n",
        "(אם n שורות ו־ slice_size = k, אז יהיו בערך n/k קריאות לקובץ. כל אחת מהן בגודל קבוע, לכן הסיבוכיות הכוללת נשארת O(n).)\n",
        "\n",
        "**מקום**: רק slice_size (שכאן בחרתי 1000) שורות נמצאות בזיכרון בכל רגע →"
      ],
      "metadata": {
        "id": "_Hazwc2BFu6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=15\n",
        "sorted_frequencies = merged_frequencies.sort_values(ascending=False)\n",
        "N_max_freq=sorted_frequencies.head(N)\n",
        "print(N_max_freq)"
      ],
      "metadata": {
        "id": "7_bbEXgopZOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**סעיף ב**"
      ],
      "metadata": {
        "id": "VFpAEps6y1h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/drive/MyDrive/hadasim task/time_series.xlsx'"
      ],
      "metadata": {
        "id": "-LD8PP45y3ra"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# קריאה של הקובץ\n",
        "df = pd.read_excel(file_path, sheet_name=\"time_series\" )  # הקובץ יקרא כגיליון שלם\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGtmWstn0_9k",
        "outputId": "85e64b0a-7fa2-4500-93cc-8cf782f80e98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            timestamp value\n",
            "0 2025-06-28 12:00:52  18.5\n",
            "1 2025-06-01 04:17:23  46.3\n",
            "2 2025-06-10 17:02:57    76\n",
            "3 2025-06-23 05:23:22  56.4\n",
            "4 2025-06-05 07:20:08  67.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "א.\tכתיבת קטע קוד המבצע בדיקות לפני עיבוד הנתונים"
      ],
      "metadata": {
        "id": "nAHO9b811oi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_data(df):\n",
        "    # המרת תאריכים לא תקניים ל-NULL\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "\n",
        "    # המרת הערכים בעמודת 'value' למספרים, והמרת ערכים לא תקניים ל-NULL\n",
        "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
        "\n",
        "    # הסרת שורות עם ערכים חסרים\n",
        "    df = df.dropna()\n",
        "\n",
        "    # הסרת כפילויות בתאריכים, כאשר שומרים את הרשומה הראשונה\n",
        "    df = df.drop_duplicates(subset='timestamp', keep='first')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "-7RxrZmLlivX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = fix_data(df)"
      ],
      "metadata": {
        "id": "J3H4pUU41kjI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ב.\tכתיבת קטע קוד המחשב את הערך הממוצע עבור כל שעה."
      ],
      "metadata": {
        "id": "D_vj_Kta8nio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hourly_avg(df):\n",
        "    # הוספת עמודה של שעה בלבד מתוך ה-Timestamp\n",
        "    df['Hour'] = df['timestamp'].dt.floor('h')\n",
        "    # קיבוץ לפי שעה וחישוב ממוצע\n",
        "    hourly_avg = df.groupby('Hour')['value'].mean().reset_index()\n",
        "\n",
        "    hourly_avg.rename(columns={'Hour': 'זמן התחלה', 'value': 'ממוצע'}, inplace=True)\n",
        "\n",
        "\n",
        "    return hourly_avg"
      ],
      "metadata": {
        "id": "IPRNSMROQkkh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# הפעלת הפונקציה\n",
        "hourly_avg = compute_hourly_avg(df)\n",
        "# הדפסת התוצאה\n",
        "print(hourly_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSLvoM_K_B0y",
        "outputId": "e185b33d-fcd7-4414-a897-4a7233e47ed4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              זמן התחלה      ממוצע\n",
            "0   2025-06-01 00:00:00  49.824309\n",
            "1   2025-06-01 01:00:00  50.564510\n",
            "2   2025-06-01 02:00:00  49.478399\n",
            "3   2025-06-01 03:00:00  50.264079\n",
            "4   2025-06-01 04:00:00  48.939780\n",
            "..                  ...        ...\n",
            "715 2025-06-30 19:00:00  51.410565\n",
            "716 2025-06-30 20:00:00  49.705429\n",
            "717 2025-06-30 21:00:00  50.631281\n",
            "718 2025-06-30 22:00:00  48.373709\n",
            "719 2025-06-30 23:00:00  50.395344\n",
            "\n",
            "[720 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.חילוק הקובץ לחלקים קטנים, חישוב עבור כל חלק ומיזוג של תוצאות החלקים"
      ],
      "metadata": {
        "id": "-b590VL3FuYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#נחלק לחלקים לפי ימים שונים\n",
        "df['Date'] = df['timestamp'].dt.date"
      ],
      "metadata": {
        "id": "Zky_a5nV_JQx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#פונקציה ליצירת קובץ csv עם הממוצעים\n",
        "def hourly_avg_to_csv(df):\n",
        "  # שמירת התוצאה בקובץ CSV\n",
        "  df.to_csv('/content/drive/MyDrive/hadasim task/hourly_avg.csv', index=False)"
      ],
      "metadata": {
        "id": "GtF7580f24gG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#יצירת data frame ריק\n",
        "#אליו נמזג את כל החלקים (הימים) השונים\n",
        "merged_parts = pd.DataFrame()\n",
        "# מעבר על כל יום בנפרד\n",
        "for date, group in df.groupby('Date'):\n",
        "    # חישוב ממוצע לשעה\n",
        "    hourly_avg = hourly_avg = compute_hourly_avg(group)\n",
        "    merged_parts = pd.concat([merged_parts, hourly_avg], ignore_index=True)\n",
        "# הצגת התוצאה\n",
        "print(merged_parts)\n",
        "#hourly_avg_to_csv(merged_parts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_hLWUKWQYOL",
        "outputId": "6d92fba7-bddf-40d5-cdbc-928aad895806"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              זמן התחלה      ממוצע\n",
            "0   2025-06-01 00:00:00  49.824309\n",
            "1   2025-06-01 01:00:00  50.564510\n",
            "2   2025-06-01 02:00:00  49.478399\n",
            "3   2025-06-01 03:00:00  50.264079\n",
            "4   2025-06-01 04:00:00  48.939780\n",
            "..                  ...        ...\n",
            "715 2025-06-30 19:00:00  51.410565\n",
            "716 2025-06-30 20:00:00  49.705429\n",
            "717 2025-06-30 21:00:00  50.631281\n",
            "718 2025-06-30 22:00:00  48.373709\n",
            "719 2025-06-30 23:00:00  50.395344\n",
            "\n",
            "[720 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.התאמת החישוב למצב של זרימת נתונים בזמן אמת:"
      ],
      "metadata": {
        "id": "pkjpen0aX5Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "עבור עדכון הממוצעים השעתיים בזמן אמת:\n",
        "בהנתן מבנה הנתונים ובו הממוצע הקיים עבור כל שעה:\n",
        "נוסיף עמודה למבנה זה ובה ישמר מספר המקורות - מספר הנתונים ממנו חושב הממוצע הקיים.\n",
        "\n",
        "כאשר נתון חדש מגיע, נעדכן את הממוצע השעתי כך:\n",
        "נכפול את הממוצע הקודם במספר המקורות הקיים ,\n",
        "נוסיף את ערך הנתון החדש.\n",
        "נחלק את התוצאה במספר המקורות החדש (שהוא מספר המקורות הנוכחי + 1).\n",
        "נעדכן את מספר המקורות על ידי הוספת 1.\n",
        "נעדכן את הממוצע של אותה השעה בממוצע החדש שקבלנו\n"
      ],
      "metadata": {
        "id": "g3pxcoDdX6mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#נשנה את הפונקציה ששימשה לנו בסעיף הקודם כך שתחשב ותשמור גם את מספר הנתונים בדאטה המתקבלת\n",
        "\n",
        "def hourly_avg_stream(df):\n",
        "    # הוספת עמודה של שעה בלבד מתוך ה-Timestamp\n",
        "    df['Hour'] = df['timestamp'].dt.floor('h')\n",
        "\n",
        "    # קיבוץ לפי שעה, חישוב ממוצע וספירת הנתונים לכל שעה\n",
        "    hourly_avg = df.groupby('Hour').agg({'value': 'mean', 'timestamp': 'count'}).reset_index()\n",
        "\n",
        "    hourly_avg.rename(columns={'Hour': 'זמן התחלה', 'value': 'ממוצע', 'timestamp': 'מספר מקורות'}, inplace=True)\n",
        "\n",
        "    return hourly_avg"
      ],
      "metadata": {
        "id": "tqwLlIgWa-aJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ניצור פונקציה המקבלת את הערכ\\ים החדש\\ים ואת המבנה נתונים שבו הממוצעים לפי שעות, מעדכנת אותו ומחזירה אותו מעודכן\n",
        "\n",
        "def hourly_update(df,new_data):\n",
        "    # המרת ה-timestamp לשעה עגולה\n",
        "    new_data['timestamp'] = pd.to_datetime(new_data['timestamp']).dt.floor('h')\n",
        "\n",
        "    #ניקח בחשבון שיכול להכנס בכל פעם נתון אחד או מספר נתונים ושהם נכנסים בצורת data frame\n",
        "    # מעבר על כל השורות ב-new_data\n",
        "    for _, row in new_data.iterrows():\n",
        "        new_hour = row['timestamp']\n",
        "        new_value = row['value']\n",
        "\n",
        "        # אם כבר יש נתון עבור השעה והתאריך האלה\n",
        "        if new_hour in df['זמן התחלה'].values:\n",
        "            # מציאת השורה המתאימה לאותה השעה\n",
        "            old_avg = df.loc[df['זמן התחלה'] == new_hour, 'ממוצע'].values[0]\n",
        "            old_count = df.loc[df['זמן התחלה'] == new_hour, 'מספר מקורות'].values[0]\n",
        "\n",
        "            # עדכון הממוצע והספירה\n",
        "            new_avg = (old_avg * old_count + new_value) / (old_count + 1)\n",
        "            df.loc[df['זמן התחלה'] == new_hour, 'ממוצע'] = new_avg\n",
        "            df.loc[df['זמן התחלה'] == new_hour, 'מספר מקורות'] = old_count + 1\n",
        "        else:\n",
        "            # אם אין נתון עבור השעה, פשוט הוסף אותו כנתון חדש\n",
        "            df = pd.concat([df, pd.DataFrame({'זמן התחלה': [new_hour], 'ממוצע': [new_value], 'מספר מקורות': [1]})], ignore_index=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "J4U1JpIDhetY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# נתון חדש (דוגמת זרימה בזמן אמת)\n",
        "new_data = pd.DataFrame({\n",
        "    'timestamp': ['2025-06-01 00:00:00', '2025-06-01 01:00:00'],\n",
        "    'value': [20.0, 30.0]\n",
        "})\n",
        "\n",
        "#נריץ בדיקה גם עבור הנתונים החדשים שהם אכן בסדר\n",
        "new_data = fix_data(new_data)\n",
        "\n",
        "#נניח שאנחנו מקבלים את df מהסעיפים הקודמים\n",
        "#הפעלת הפונקציה המחשבת את הערך הממוצע עבור כל שעה וסופרת את הנתונים\n",
        "hourly_avg=hourly_avg_stream(df)\n",
        "#print(hourly_avg.head())\n",
        "#עדכון הממוצעים הכולל את הדאטה החדשה\n",
        "hourly_avg=hourly_update(hourly_avg,new_data)\n",
        "\n",
        "#נרצה לקבל רק את נתוני הממוצעים לפי שעה ובלי מספר המקורות..\n",
        "hourly_avg_neto = hourly_avg.drop(columns=['מספר מקורות'])\n",
        "print(hourly_avg_neto)\n",
        "#hourly_avg_to_csv(hourly_avg_neto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEiugLWIencM",
        "outputId": "a223afd3-9748-453c-df90-ad7e532b18ca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              זמן התחלה      ממוצע\n",
            "0   2025-06-01 00:00:00  49.795905\n",
            "1   2025-06-01 01:00:00  50.544962\n",
            "2   2025-06-01 02:00:00  49.478399\n",
            "3   2025-06-01 03:00:00  50.264079\n",
            "4   2025-06-01 04:00:00  48.939780\n",
            "..                  ...        ...\n",
            "715 2025-06-30 19:00:00  51.410565\n",
            "716 2025-06-30 20:00:00  49.705429\n",
            "717 2025-06-30 21:00:00  50.631281\n",
            "718 2025-06-30 22:00:00  48.373709\n",
            "719 2025-06-30 23:00:00  50.395344\n",
            "\n",
            "[720 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.התאמת הקוד לפורמט PARQUET"
      ],
      "metadata": {
        "id": "UC_mN_ANrkuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path='/content/drive/MyDrive/hadasim task/time_series (4).parquet'"
      ],
      "metadata": {
        "id": "KZarXQyormCP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# קריאה של הקובץ\n",
        "df = pd.read_parquet(file_path )\n",
        "#print(df)"
      ],
      "metadata": {
        "id": "yp7rKF5Ar1RT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "נשתמש בסעיף 3 כך שיתאים גם לאפשרות שיש זרימה בזמן אמת, מה שגם מתאים לנו מבחינת העובדה שנתונים כבר הממוצע ו\\מספר המקורות"
      ],
      "metadata": {
        "id": "HN-ZRlj0uOjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#נשמור את העמודות נחוצות לנו\n",
        "#נשנה את שם עמודת הממוצע כדי שיתאים לפונקציית בדיקת הנתונים\n",
        "df = df[['timestamp', 'mean_value', 'count']].rename(columns={'mean_value': 'value'})\n",
        "#print(df.head())\n",
        "\n",
        "#נריץ בדיקה  עבור הנתונים שהם אכן בסדר\n",
        "df = fix_data(df)\n",
        "\n",
        "#כעת נשנה את שמות העמודות כך שיתאימו לפונקציות ושיודפסו כמו שרצוי\n",
        "df.rename(columns={'timestamp': 'זמן התחלה', 'value': 'ממוצע', 'count': 'מספר מקורות'}, inplace=True)\n",
        "#print(df.head())\n"
      ],
      "metadata": {
        "id": "SfR8WFPjuLFu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# נתון חדש (דוגמת זרימה בזמן אמת)\n",
        "#new_data = pd.DataFrame({\n",
        "#    'timestamp': ['2025-06-01 00:00:00', '2025-06-01 01:00:00'],\n",
        "#    'value': [20.0, 30.0]\n",
        "#})\n",
        "\n",
        "#נריץ בדיקה גם עבור הנתונים החדשים שהם אכן בסדר\n",
        "#new_data = fix_data(new_data)\n",
        "\n",
        "#נניח שאנחנו מקבלים את df מהסעיפים הקודמים\n",
        "#הפעלת הפונקציה המחשבת את הערך הממוצע עבור כל שעה וסופרת את הנתונים\n",
        "#hourly_avg=hourly_avg_stream(df)\n",
        "#print(hourly_avg.head())\n",
        "#עדכון הממוצעים הכולל את הדאטה החדשה\n",
        "#hourly_avg=hourly_update(hourly_avg,new_data)\n"
      ],
      "metadata": {
        "id": "CBhMzfeNTAX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#נרצה לקבל רק את נתוני הממוצעים לפי שעה ובלי מספר המקורות..\n",
        "hourly_avg_neto = df.drop(columns=['מספר מקורות'])\n",
        "print(hourly_avg_neto)\n",
        "hourly_avg_to_csv(hourly_avg_neto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNxMF9iYx0mY",
        "outputId": "ebb9a772-8066-4674-cfc0-f91cf0777a21"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              זמן התחלה      ממוצע\n",
            "0   2025-06-01 00:00:00  49.985786\n",
            "1   2025-06-01 01:00:00  50.492752\n",
            "2   2025-06-01 02:00:00  49.627839\n",
            "3   2025-06-01 03:00:00  50.092934\n",
            "4   2025-06-01 04:00:00  49.514965\n",
            "..                  ...        ...\n",
            "691 2025-06-29 19:00:00  50.244835\n",
            "692 2025-06-29 20:00:00  49.963043\n",
            "693 2025-06-29 21:00:00  49.703735\n",
            "694 2025-06-29 22:00:00  49.816577\n",
            "695 2025-06-29 23:00:00  49.782904\n",
            "\n",
            "[696 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " היתרונות באחסון המידע בפורמט הנתון(PARQUET):\n",
        "\n",
        " -כמו שניתן לראות גודל קובץ הPARQUET הוא מזערי לעומת גודל קובץ הXLSX\n",
        "מה שאומר שיש לו יכולת דחיסה הרבה יותר טובה\n",
        " -כמו שעוד ניתן לראות קריאת הנתונים מהקובץ נעשית במהירות גדולה יותר מאשר קריאת הנתונים מקובץ האקסל\n",
        "\n",
        " באופן כללי עוד מיתרונות קובץ PARQUET:\n",
        " הנתונים נשמרים לפי סוגם: טקסט, מספרים וכו\n",
        " הנתונים נשמרים בעמודות (שלא לדוגמה כמו באקסל ששומר בשורות מה שגורם לזמן רב יותר הנדרש לקריאת הנתונים)\n",
        "\n",
        "לסיכום:\n",
        " פורמט PARQUET טוב לחיסכון במקום, לשליפה מהירה של נתונים ושמירה על דיוקם. ובעצם מתאים לעבודה עם מאגרי נתונים גדולים."
      ],
      "metadata": {
        "id": "t_brz3WZyiuc"
      }
    }
  ]
}